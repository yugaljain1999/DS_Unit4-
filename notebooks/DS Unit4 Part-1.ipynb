{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Prerequesties\n",
    "!pip install -r requirements.txt\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Here we will see how to tokenize the text i.e. divide whole text into smaller chunks either chunks of sentences or chunks of words. There are two ways of tokenization i.e. sentence tokenization and word tokenization. Let's go through one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.', 'Challenges in natural language processing frequently involve natural language understanding, natural languagegeneration frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.'] \n",
      " ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'languagegeneration', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.']\n"
     ]
    }
   ],
   "source": [
    "# Here we are considering sample corpus for sentence tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "\n",
    "text = \"Natural language processing (NLP) is a field \" + \\\n",
    "       \"of computer science, artificial intelligence \" + \\\n",
    "       \"and computational linguistics concerned with \" + \\\n",
    "       \"the interactions between computers and human \" + \\\n",
    "       \"(natural) languages, and, in particular, \" + \\\n",
    "       \"concerned with programming computers to \" + \\\n",
    "       \"fruitfully process large natural language \" + \\\n",
    "       \"corpora. Challenges in natural language \" + \\\n",
    "       \"processing frequently involve natural \" + \\\n",
    "       \"language understanding, natural language\" + \\\n",
    "       \"generation frequently from formal, machine\" + \\\n",
    "       \"-readable logical forms), connecting language \" + \\\n",
    "       \"and machine perception, managing human-\" + \\\n",
    "       \"computer dialog systems, or some combination \" + \\\n",
    "       \"thereof.\"\n",
    "\n",
    "tokenized_sents = sent_tokenize(text)\n",
    "tokenized_words = word_tokenize(text)\n",
    "\n",
    "print(tokenized_sents,'\\n',tokenized_words)\n",
    "\n",
    "# As we can see in output, sentence is tokenized based on period(.) and words are tokenized based on space between two words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stop words from a list of tokens\n",
    "Stopwords are the words which doesn't make that much sense when we play with sentences. For e.g the, has, have etc. So Let's remove it from list of tokens we generated above to cut down training time of building of machine learning model for downstream tasks.\n",
    "\n",
    "Let's import stopwords using nltk library and filter from list of word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('English'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'field', 'computer', 'science', ',', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'interactions', 'computers', 'human', '(', 'natural', ')', 'languages', ',', ',', 'particular', ',', 'concerned', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'Challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'languagegeneration', 'frequently', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'combination', 'thereof', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentences = [word for word in tokenized_words if not word.lower() in stop_words]\n",
    "print(filtered_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "Stemming or lemmatization means generation of root form of inflected words. For e.g charge is derived by stemming or lemmatization of charging,charges etc.\n",
    "\n",
    "* There is little theoritical difference between these two i.e stemming may not result actual word but lemmatization will result actual word as root. So we can say that lemmatization is specialized form of stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program\n",
      "program\n",
      "charg\n",
      "studi\n",
      "code\n"
     ]
    }
   ],
   "source": [
    "# Stemming using NLTK\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "words = ['Programs','Programming','Charging','Studying','Coding']\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock\n",
      "program\n",
      "guest\n",
      "game\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization using NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "words_ = ['rocks','programs','guests','games']\n",
    "for w in words_:\n",
    "    print(wnl.lemmatize(w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Represent document as vector\n",
    "Let's say we have bunch of sentences in a document and we wanna do classify texts then how we can fed input to the machine learning model. As system understands numeric values, we need to convert text into numeric as feature vectors representation so that it can be fed to model.\n",
    "\n",
    "There are different methods of vectorization like countvectorizer(i.e. counts the occurences of word in a document) and tfidf(Term frequency inverse document frequency) vectorizer.\n",
    "\n",
    "Let's use scikit-learn library to import vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before vectorization\n",
      "['.@AshishXL Bhai, Plasma donation in this case, has been done, thanks to the donor and our health team (Santosh, Kiran, Sirtaj, Gurpeet)ðŸ™ðŸ¼\\n#StayHome #DelhiFightsCorona \\n@sharmanagendar https://t.co/MGXZv3DzO5', 'nan', 'Impact of COVID-19 on pregnancy \\n#Obstetrics #Maternalhealth #childlife #UNICEF \\n\\nhttps://t.co/vIjP4gF9HT', \"China's cover-up of the Wuhan virus allowed the disease to spread all over the world, instigating a global pandemic that has caused over 100000 American lives &amp; over a million lives worldwide. Chinese officials ignored their reporting obligations to WHO: US President Donald Trump https://t.co/jo15L2s38C\", '#COVID19 recovery rate improves to around 43 per cent; over 71,000 cured so far\\n\\nhttps://t.co/dO48FjSEyb', 'BREAKING:\\n\\nPresident Trump just announced that the US will be TERMINATING our relationship with the WHOâ€”the Wuhan Health Organization!\\n\\nRT if you are THRILLED this is finally happening!', '#IndiaFightsCorona \\n\\nLet us all boost the morale of people who have recovered from #COVID19. They need our support. Together we will fight #COVID19. \\n\\n#TiraskarNahiTilakKaro #HealthForAll #SwasthaBharat #CoronaOutbreak #Lockdown4 https://t.co/LgnxMyS4Ls', 'Despite reports to the contrary, Sweden is paying heavily for its decision not to lockdown. As of today, 2462 people have died there, a much higher number than the neighboring countries of Norway (207), Finland (206) or Denmark (443). The United States made the correct decision!', \"#BigBreakingNews | In today's #WhiteHouse speech, #US President #DonaldTrump says #USA is ending its relationship with #WorldHealthOrganization (#WHO). | #BreakingNews @Breaking_24X7 #TrumpDepression #CoronaUpdates #CoronaVirusUpdates #CoronaVirus #CoronaPandemic @BengalNewzWorld\", '28 May\\n\\nðŸ”¹ 1024  NEW #Corona cases in Delhi in the last 24 hours, TOTAL- 16,281 cases.\\n\\nðŸ”¹ 231 Recovered today, TOTAL Recovered - 7495.\\n\\nðŸ”¹ Death toll rises to 316.\\n    \\nðŸ”¹ Number of active cases is 8470.\\n\\n#DelhiFightsCorona https://t.co/w6jjgNAFhi']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Let's play with sample tweets dataset\n",
    "data = pd.read_csv('tweets_2020-05-29-20.csv')\n",
    "docs = list(data['full_retweet_text'][:10])\n",
    "\n",
    "docs = [str(sent) for sent in docs]\n",
    "print('Before vectorization')\n",
    "print(docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorization\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer\n",
    "count_vectorizer = CountVectorizer() # We can tweek some parameters like ngram_range and analyzer here - ngram_range is bydefault (1,1) else if we want pairs of words count then it can be (2,2) and so on\n",
    "vectors = count_vectorizer.fit_transform(docs)\n",
    "\n",
    "\n",
    "print('After vectorization')\n",
    "print(vectors.toarray())\n",
    "\n",
    "# Here this count vectorizer depicts the count of particular word in a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectors [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Features names ['000' '000 cured' '100000' '100000 american' '1024' '1024 new' '16'\n",
      " '16 281' '19' '19 on' '206' '206 or' '207' '207 finland' '231'\n",
      " '231 recovered' '24' '24 hours' '2462' '2462 people' '28' '28 may' '281'\n",
      " '281 cases' '316' '316 number' '43' '43 per' '443' '443 the' '71'\n",
      " '71 000' '7495' '7495 death' '8470' '8470 delhifightscorona' 'active'\n",
      " 'active cases' 'all' 'all boost' 'all over' 'allowed' 'allowed the'\n",
      " 'american' 'american lives' 'amp' 'amp over' 'and' 'and our' 'announced'\n",
      " 'announced that' 'are' 'are thrilled' 'around' 'around 43' 'as' 'as of'\n",
      " 'ashishxl' 'ashishxl bhai' 'be' 'be terminating' 'been' 'been done'\n",
      " 'bengalnewzworld' 'bhai' 'bhai plasma' 'bigbreakingnews'\n",
      " 'bigbreakingnews in' 'boost' 'boost the' 'breaking' 'breaking president'\n",
      " 'breaking_24x7' 'breaking_24x7 trumpdepression' 'breakingnews'\n",
      " 'breakingnews breaking_24x7' 'case' 'case has' 'cases' 'cases 231'\n",
      " 'cases in' 'cases is' 'caused' 'caused over' 'cent' 'cent over'\n",
      " 'childlife' 'childlife unicef' 'china' 'china cover' 'chinese'\n",
      " 'chinese officials' 'co' 'co do48fjseyb' 'co jo15l2s38c' 'co lgnxmys4ls'\n",
      " 'co mgxzv3dzo5' 'co vijp4gf9ht' 'co w6jjgnafhi' 'contrary'\n",
      " 'contrary sweden' 'corona' 'corona cases' 'coronaoutbreak'\n",
      " 'coronaoutbreak lockdown4' 'coronapandemic'\n",
      " 'coronapandemic bengalnewzworld' 'coronaupdates'\n",
      " 'coronaupdates coronavirusupdates' 'coronavirus'\n",
      " 'coronavirus coronapandemic' 'coronavirusupdates'\n",
      " 'coronavirusupdates coronavirus' 'correct' 'correct decision' 'countries'\n",
      " 'countries of' 'cover' 'cover up' 'covid' 'covid 19' 'covid19'\n",
      " 'covid19 recovery' 'covid19 they' 'covid19 tiraskarnahitilakkaro' 'cured'\n",
      " 'cured so' 'death' 'death toll' 'decision' 'decision not' 'delhi'\n",
      " 'delhi in' 'delhifightscorona' 'delhifightscorona https'\n",
      " 'delhifightscorona sharmanagendar' 'denmark' 'denmark 443' 'despite'\n",
      " 'despite reports' 'died' 'died there' 'disease' 'disease to' 'do48fjseyb'\n",
      " 'donald' 'donald trump' 'donaldtrump' 'donaldtrump says' 'donation'\n",
      " 'donation in' 'done' 'done thanks' 'donor' 'donor and' 'ending'\n",
      " 'ending its' 'far' 'far https' 'fight' 'fight covid19' 'finally'\n",
      " 'finally happening' 'finland' 'finland 206' 'for' 'for its' 'from'\n",
      " 'from covid19' 'global' 'global pandemic' 'gurpeet' 'gurpeet stayhome'\n",
      " 'happening' 'has' 'has been' 'has caused' 'have' 'have died'\n",
      " 'have recovered' 'health' 'health organization' 'health team'\n",
      " 'healthforall' 'healthforall swasthabharat' 'heavily' 'heavily for'\n",
      " 'higher' 'higher number' 'hours' 'hours total' 'https' 'https co' 'if'\n",
      " 'if you' 'ignored' 'ignored their' 'impact' 'impact of' 'improves'\n",
      " 'improves to' 'in' 'in delhi' 'in the' 'in this' 'in today'\n",
      " 'indiafightscorona' 'indiafightscorona let' 'instigating'\n",
      " 'instigating global' 'is' 'is 8470' 'is ending' 'is finally' 'is paying'\n",
      " 'its' 'its decision' 'its relationship' 'jo15l2s38c' 'just'\n",
      " 'just announced' 'kiran' 'kiran sirtaj' 'last' 'last 24' 'let' 'let us'\n",
      " 'lgnxmys4ls' 'lives' 'lives amp' 'lives worldwide' 'lockdown'\n",
      " 'lockdown as' 'lockdown4' 'lockdown4 https' 'made' 'made the'\n",
      " 'maternalhealth' 'maternalhealth childlife' 'may' 'may 1024' 'mgxzv3dzo5'\n",
      " 'million' 'million lives' 'morale' 'morale of' 'much' 'much higher' 'nan'\n",
      " 'need' 'need our' 'neighboring' 'neighboring countries' 'new'\n",
      " 'new corona' 'norway' 'norway 207' 'not' 'not to' 'number' 'number of'\n",
      " 'number than' 'obligations' 'obligations to' 'obstetrics'\n",
      " 'obstetrics maternalhealth' 'of' 'of active' 'of covid' 'of norway'\n",
      " 'of people' 'of the' 'of today' 'officials' 'officials ignored' 'on'\n",
      " 'on pregnancy' 'or' 'or denmark' 'organization' 'organization rt' 'our'\n",
      " 'our health' 'our relationship' 'our support' 'over' 'over 100000'\n",
      " 'over 71' 'over million' 'over the' 'pandemic' 'pandemic that' 'paying'\n",
      " 'paying heavily' 'people' 'people have' 'people who' 'per' 'per cent'\n",
      " 'plasma' 'plasma donation' 'pregnancy' 'pregnancy obstetrics' 'president'\n",
      " 'president donald' 'president donaldtrump' 'president trump' 'rate'\n",
      " 'rate improves' 'recovered' 'recovered 7495' 'recovered from'\n",
      " 'recovered today' 'recovery' 'recovery rate' 'relationship'\n",
      " 'relationship with' 'reporting' 'reporting obligations' 'reports'\n",
      " 'reports to' 'rises' 'rises to' 'rt' 'rt if' 'santosh' 'santosh kiran'\n",
      " 'says' 'says usa' 'sharmanagendar' 'sharmanagendar https' 'sirtaj'\n",
      " 'sirtaj gurpeet' 'so' 'so far' 'speech' 'speech us' 'spread' 'spread all'\n",
      " 'states' 'states made' 'stayhome' 'stayhome delhifightscorona' 'support'\n",
      " 'support together' 'swasthabharat' 'swasthabharat coronaoutbreak'\n",
      " 'sweden' 'sweden is' 'team' 'team santosh' 'terminating'\n",
      " 'terminating our' 'than' 'than the' 'thanks' 'thanks to' 'that'\n",
      " 'that has' 'that the' 'the' 'the contrary' 'the correct' 'the disease'\n",
      " 'the donor' 'the last' 'the morale' 'the neighboring' 'the united'\n",
      " 'the us' 'the who' 'the world' 'the wuhan' 'their' 'their reporting'\n",
      " 'there' 'there much' 'they' 'they need' 'this' 'this case' 'this is'\n",
      " 'thrilled' 'thrilled this' 'tiraskarnahitilakkaro'\n",
      " 'tiraskarnahitilakkaro healthforall' 'to' 'to 316' 'to around'\n",
      " 'to lockdown' 'to spread' 'to the' 'to who' 'today' 'today 2462'\n",
      " 'today total' 'today whitehouse' 'together' 'together we' 'toll'\n",
      " 'toll rises' 'total' 'total 16' 'total recovered' 'trump' 'trump https'\n",
      " 'trump just' 'trumpdepression' 'trumpdepression coronaupdates' 'unicef'\n",
      " 'unicef https' 'united' 'united states' 'up' 'up of' 'us' 'us all'\n",
      " 'us president' 'us will' 'usa' 'usa is' 'vijp4gf9ht' 'virus'\n",
      " 'virus allowed' 'w6jjgnafhi' 'we' 'we will' 'whitehouse'\n",
      " 'whitehouse speech' 'who' 'who breakingnews' 'who have' 'who the'\n",
      " 'who us' 'will' 'will be' 'will fight' 'with' 'with the'\n",
      " 'with worldhealthorganization' 'world' 'world instigating'\n",
      " 'worldhealthorganization' 'worldhealthorganization who' 'worldwide'\n",
      " 'worldwide chinese' 'wuhan' 'wuhan health' 'wuhan virus' 'you' 'you are']\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(use_idf = True, analyzer = 'word', ngram_range=(1,2))\n",
    "tfidf_vectorizer.fit(docs)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(docs)\n",
    "print('TF-IDF Vectors', tfidf_vectors.toarray())\n",
    "print('Features names', tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query documents by similarity\n",
    "\n",
    "We can use cosine similarity scores to find similar sentences in a document after TF-IDF vectorization.\n",
    "\n",
    "Note- TF-IDF vectorization works better than Count vectorization as prior considers frequency of words in document and number of documents containing those words as well which helps to determine similarity between individual sentences in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.02614298 0.04764016 0.03083108 0.05700748\n",
      "  0.03265538 0.03920179 0.01180718 0.05367363]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Let's consider first sentence in docs as query sentence\n",
    "query = tfidf_vectors[0:1]\n",
    "similarity_matrix = cosine_similarity(query,tfidf_vectors.toarray())\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Word Embeddings Model to create Document Vectors\n",
    "\n",
    "Let's explore another method to create document vectors i.e. pretrained word embeddings models to get contextual features from given text sequences. These document vectors can help to classify sentences and many other downstream tasks further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.15237004,  0.18714751, -0.5484187 ,  0.30625474,  1.6435678 ,\n",
      "        0.7385486 ,  0.45245537, -0.15774831,  0.99820447,  1.5292078 ,\n",
      "        0.6275515 , -1.1710415 , -0.55321056, -0.942731  , -0.6987732 ,\n",
      "       -1.3398434 , -0.2996181 ,  0.32035145,  0.10361589, -0.5445901 ,\n",
      "        0.6154242 , -0.3586897 , -0.31722206, -0.19259013, -0.511952  ,\n",
      "       -0.3073782 , -0.5385157 , -0.69947106,  0.7838872 , -0.58011055,\n",
      "        0.05466099,  0.5952998 ,  0.2915551 ,  0.18558408,  0.29152414,\n",
      "       -0.33659157,  1.5055457 , -0.9074809 ,  0.05406311, -0.6646737 ,\n",
      "        1.3476272 ,  0.49729127,  0.21240169, -1.8145515 , -1.2545425 ,\n",
      "       -0.89764935,  0.09181776, -0.3249551 , -0.7165451 ,  0.52670294,\n",
      "        0.55528575, -1.1691321 , -0.5744502 ,  0.00452233, -1.8886673 ,\n",
      "        0.8994386 ,  0.23017785,  0.52385014, -0.3442439 , -0.07341588,\n",
      "       -0.16352   , -0.36046058,  1.0966384 ,  0.52782434,  0.06773804,\n",
      "       -0.17929795,  0.58979917, -1.109526  , -0.90127087,  0.67461634,\n",
      "        0.68218845, -0.27587438,  0.31658158,  0.69867414, -0.5235036 ,\n",
      "        0.90504164,  0.65798163,  0.31150284, -0.13723731,  0.19043115,\n",
      "        0.5982728 , -0.44950563,  0.37700334, -0.40018374,  0.64086795,\n",
      "        0.48938987,  0.17998475,  0.33721963, -0.03645489,  0.5491787 ,\n",
      "       -0.01304418, -0.31964496, -0.39497674,  0.37791532,  1.1055605 ,\n",
      "        0.17858383], dtype=float32), array([ 2.4005961 , -3.6389556 , -0.3088795 ,  1.9503659 ,  4.815768  ,\n",
      "        2.5586023 ,  2.1736615 ,  1.1669066 ,  2.909469  ,  2.2249684 ,\n",
      "        1.7224121 , -1.1414961 , -0.5279875 ,  1.2771642 , -1.1057093 ,\n",
      "       -0.04458898, -2.6394632 ,  3.736185  ,  1.223527  ,  1.3528528 ,\n",
      "       -2.9257708 ,  0.2689919 , -1.4027698 , -2.7161183 ,  1.944308  ,\n",
      "       -3.511669  ,  2.7591293 , -2.1701157 ,  0.7824333 , -2.8848348 ,\n",
      "       -0.5017753 , -2.4232922 ,  0.25362778,  3.6065416 ,  1.9490063 ,\n",
      "       -2.9880035 ,  0.278602  ,  1.0772626 , -2.2753904 ,  1.6246594 ,\n",
      "        3.6708736 ,  1.5091531 , -0.84850955, -3.0823588 , -1.980895  ,\n",
      "        0.97793794,  0.806632  , -1.3430365 , -2.5295403 ,  0.11100668,\n",
      "       -1.7525725 , -1.595514  , -0.1955086 , -2.0796485 , -3.2026737 ,\n",
      "       -0.18165842,  1.1941738 ,  5.8202877 ,  0.61359173, -1.166348  ,\n",
      "       -1.3707231 ,  0.915533  , -0.62192684, -1.707658  ,  2.0618742 ,\n",
      "       -1.1793101 , -1.2804155 , -2.2268157 , -1.0865929 , -1.0371721 ,\n",
      "       -1.7193561 , -0.9562007 ,  1.9606073 ,  3.836698  , -1.7990911 ,\n",
      "       -0.8313705 ,  0.3577131 ,  1.4859991 , -1.6596216 ,  1.9951584 ,\n",
      "        0.31984308, -2.933923  , -1.9003577 , -1.9628222 ,  7.768085  ,\n",
      "        5.5531845 , -0.6001345 , -0.50382686, -1.3233765 , -1.7701299 ,\n",
      "       -0.59509057, -0.07424915, -1.1153188 ,  3.622401  , -0.42235056,\n",
      "       -2.104975  ], dtype=float32), array([-4.50128257e-01, -6.26546681e-01, -1.80059165e-01,  4.86480862e-01,\n",
      "        2.69276649e-03,  8.88264656e-01,  3.93212646e-01,  8.33525419e-01,\n",
      "        3.02996665e-01,  8.96528244e-01,  1.15621960e+00, -7.34980226e-01,\n",
      "        1.03648579e+00, -1.14003134e+00, -9.65610623e-01,  3.48297656e-02,\n",
      "       -7.34559298e-01, -1.36901438e-03, -3.65747362e-01, -1.26029059e-01,\n",
      "        1.03551733e+00, -3.34031254e-01,  1.12598050e+00, -3.20799127e-02,\n",
      "       -1.68853903e+00, -3.97540629e-02, -8.08959305e-01, -1.02455422e-01,\n",
      "        1.15141797e+00, -8.79710138e-01,  1.21907696e-01,  1.00944340e+00,\n",
      "        5.60801566e-01, -7.67025471e-01, -5.99539220e-01, -5.46399057e-01,\n",
      "        1.27589786e+00, -1.89761198e+00, -7.24555969e-01, -3.38296950e-01,\n",
      "        2.34731746e+00, -6.65633500e-01,  3.44429493e-01, -1.84034789e+00,\n",
      "       -9.03081596e-01, -4.84152108e-01, -4.83775944e-01, -1.11018693e+00,\n",
      "       -1.05635834e+00,  4.98497427e-01,  6.99411333e-01, -1.09242058e+00,\n",
      "       -1.05979609e+00, -5.32786697e-02, -2.40559030e+00,  1.10184062e+00,\n",
      "        3.91503572e-01,  1.13528836e+00,  3.45039777e-02,  7.16231704e-01,\n",
      "        1.31595552e+00,  7.51799494e-02,  1.78983831e+00,  6.66764736e-01,\n",
      "        6.03508115e-01, -1.08144736e+00, -8.35344642e-02, -1.79610252e+00,\n",
      "       -1.06829596e+00,  2.73564279e-01,  9.36990976e-03, -1.02714753e+00,\n",
      "        2.50362158e+00,  2.55131423e-01, -4.36399639e-01,  6.31257415e-01,\n",
      "        1.30051470e+00,  4.85132366e-01,  5.09019554e-01,  2.41645038e-01,\n",
      "        1.47193575e+00, -4.78191465e-01,  3.14072430e-01, -2.47480869e-01,\n",
      "        6.33013844e-01,  3.22659075e-01,  2.38577813e-01,  1.90524057e-01,\n",
      "       -3.15798700e-01,  1.33662856e+00,  4.09160942e-01, -5.22824764e-01,\n",
      "       -4.51976240e-01, -8.06951106e-01,  2.99523616e+00,  2.49067515e-01],\n",
      "      dtype=float32), array([ 0.08128265,  0.13654931, -0.43133673,  0.05468639,  0.8262281 ,\n",
      "        0.11206481,  0.46551895, -0.28125423,  1.0875218 ,  1.6886601 ,\n",
      "       -0.28190073,  0.24432531, -0.5753303 , -0.70951015, -1.2414105 ,\n",
      "       -0.7356458 , -0.6912474 ,  0.11845709, -0.45044586, -0.06622653,\n",
      "        0.00325564, -0.3301301 , -0.2877453 , -0.17371991, -0.5447297 ,\n",
      "        0.07279214, -1.2952844 , -0.197552  ,  0.0768946 , -1.0038525 ,\n",
      "        0.9631511 ,  0.13883589, -0.21371254, -0.36621764,  0.10411055,\n",
      "       -1.1943853 ,  0.55213374, -0.48261088, -1.0650558 , -0.01865466,\n",
      "        1.1659431 ,  0.00539099,  0.24993666, -0.91296005,  0.49156338,\n",
      "       -0.07158038,  0.2702874 , -0.20426768, -0.45555353,  0.5952926 ,\n",
      "        0.7204589 , -0.99964327,  0.19119652, -0.2418318 , -1.1677808 ,\n",
      "        1.1639502 ,  0.45649904,  0.46464896,  0.45566356, -0.25674778,\n",
      "        0.34108862, -0.46487343,  1.6104063 ,  0.31594166,  0.6874063 ,\n",
      "       -0.16434817,  1.5627455 , -1.1551679 , -0.1792221 ,  0.72116965,\n",
      "        0.4576858 , -0.59916264,  0.38441288, -0.24543752, -0.2304848 ,\n",
      "        0.03285608,  1.7604629 , -0.697488  , -0.6158377 , -0.36075085,\n",
      "        0.13787158, -0.7363398 ,  0.626613  , -0.21735896,  0.8741416 ,\n",
      "       -0.23054147,  0.4693726 ,  0.40948278, -0.30559507,  0.44612485,\n",
      "       -0.4078393 , -0.7118148 ,  0.31093737,  0.6512299 ,  0.6019548 ,\n",
      "        1.2107532 ], dtype=float32), array([ 0.88994473,  0.20657025,  0.8295093 ,  0.3871264 ,  0.3843545 ,\n",
      "        0.44625112,  1.1395777 ,  1.1432362 , -0.08166078,  1.6736813 ,\n",
      "       -0.9488487 ,  0.4607187 , -0.8223744 , -1.0722194 , -0.938211  ,\n",
      "       -0.23504585, -1.6711842 , -0.5458423 , -0.9284823 ,  0.8015581 ,\n",
      "        0.7866767 , -0.43110877, -0.412387  ,  0.2654017 , -1.1721323 ,\n",
      "        0.8111819 , -0.75103515, -0.12702091,  0.93742657, -0.8207643 ,\n",
      "        0.18133594, -0.33387965,  0.5557954 , -0.1513883 , -1.1459383 ,\n",
      "       -1.2488288 ,  0.28169554, -0.6747591 , -0.9266013 , -0.07372421,\n",
      "        0.91528976,  0.23465858, -0.24737369, -1.4510999 , -0.12600178,\n",
      "        0.39355898,  0.60339487, -0.56377506, -0.66345084, -0.01912036,\n",
      "        0.7821342 , -1.3133906 ,  0.14467695,  1.116842  , -1.1272717 ,\n",
      "        0.66019636,  0.9526544 ,  0.3239774 ,  0.6930125 ,  0.13991544,\n",
      "        0.45089832, -0.3344623 ,  1.7854953 , -0.6945478 ,  1.0754317 ,\n",
      "       -0.7172668 ,  1.4979397 , -1.3547338 , -0.74154973,  1.0071455 ,\n",
      "        0.7018745 , -1.3793514 ,  1.7289008 ,  0.7145063 , -0.32235104,\n",
      "       -0.2821503 ,  2.3845391 ,  0.6243527 ,  0.14147455, -0.30430397,\n",
      "       -0.16134042, -1.9706622 ,  0.88864434, -0.21935484,  0.00825559,\n",
      "       -0.3607021 ,  0.07084944,  0.613673  , -0.27315032,  0.25633895,\n",
      "       -0.65884095, -1.4425073 , -0.48828813, -0.22676948,  1.4096036 ,\n",
      "        0.82915473], dtype=float32), array([ 0.35956594,  0.40219688, -0.07230198, -0.34338182,  0.84483045,\n",
      "        1.2883028 ,  0.9167695 , -0.05018629,  0.8401332 ,  0.93039477,\n",
      "       -0.10944615, -0.4602877 , -0.50930053, -0.01322047, -0.8799753 ,\n",
      "       -0.707245  , -0.20086001,  0.8329721 , -0.487492  , -0.80733496,\n",
      "       -0.26215422, -0.02790237, -0.05648409, -0.81543463, -0.7912518 ,\n",
      "        0.68710077, -0.9613725 , -0.91339225, -0.2885375 , -0.6676398 ,\n",
      "        0.87934643,  0.53407514, -0.08582394, -0.29854155,  0.25880575,\n",
      "       -1.272554  ,  0.7241517 , -0.7908654 , -0.6518971 , -0.5015369 ,\n",
      "        1.2547753 ,  0.09044173,  0.62899303, -0.73352474,  0.22692837,\n",
      "        0.08777858, -0.2715565 ,  0.08025667, -0.47890308,  0.00410002,\n",
      "        0.8671622 , -0.52899945, -0.02133579,  0.09358399, -0.832028  ,\n",
      "        0.45319492,  0.701377  ,  0.7101069 ,  0.7142101 , -0.5376446 ,\n",
      "        0.75143737, -0.4301677 ,  0.6433937 ,  0.5658325 ,  0.17847274,\n",
      "       -0.01031467, -0.5351619 , -0.84599334, -0.23087491,  0.22734396,\n",
      "       -0.03157126,  0.13147488,  0.27784103, -0.5458607 ,  0.3062544 ,\n",
      "        0.12795025,  1.4179435 , -0.26422268, -0.91332555, -0.3810185 ,\n",
      "        0.5461659 , -0.43698898,  0.22097936,  0.06547787,  1.588989  ,\n",
      "        0.16269809,  0.55205566,  0.2958651 ,  0.19663766,  0.3934709 ,\n",
      "       -0.91525835, -0.5868106 , -0.17516117,  1.0907074 ,  0.43239006,\n",
      "        0.1912694 ], dtype=float32), array([-0.26480654, -0.02778441, -0.12771694, -0.34106278,  0.37214008,\n",
      "        0.90482175,  0.7001378 ,  0.56081825,  1.6902049 ,  0.34732863,\n",
      "       -0.38740402,  0.1717446 ,  0.21314998, -0.6482243 , -0.30371463,\n",
      "       -0.38304886, -0.8239293 , -0.41286746, -0.10184513,  0.24242666,\n",
      "       -0.15596852, -0.25535884,  0.08622407, -0.8479694 , -1.3444811 ,\n",
      "       -0.01882113, -0.03590049, -0.44636858,  0.31039852, -0.85055107,\n",
      "        0.79813176,  0.12378174,  0.5386306 , -0.9040668 , -0.16723295,\n",
      "       -0.27234876,  0.5299099 , -1.1612692 , -0.20660093, -0.03843357,\n",
      "        0.8134009 , -0.37149185,  0.86678225, -1.3919091 ,  0.08539579,\n",
      "        0.9709479 , -0.13677219, -0.26728016, -0.5767735 ,  0.75786304,\n",
      "        0.61018336, -0.85443413, -0.2972985 ,  0.52534366, -1.4737146 ,\n",
      "        0.5662542 ,  0.68890536,  0.41816688,  0.27499682,  0.98524356,\n",
      "        1.0113055 , -0.7976175 ,  1.163185  ,  0.7698949 ,  0.10068108,\n",
      "       -0.7680532 , -0.4709579 , -1.009048  , -1.1890532 , -0.04723784,\n",
      "       -0.19502193,  0.24631032,  1.2363273 ,  0.07603446, -0.33596277,\n",
      "        0.8461815 ,  1.5115021 ,  0.200163  , -0.14954972,  0.10983027,\n",
      "        0.9679177 , -0.6528113 ,  0.28807878, -0.2248696 ,  0.9299911 ,\n",
      "        0.27317604,  0.17397283,  0.13290775, -0.3129582 ,  0.45702252,\n",
      "       -0.6704395 , -0.5315072 ,  0.12474272, -0.07884618,  1.0920608 ,\n",
      "        0.1589994 ], dtype=float32), array([ 0.12744123,  0.56617457,  0.2919738 ,  0.19723684,  2.112856  ,\n",
      "        0.05004992,  0.9142553 , -0.35842326,  1.0341641 ,  0.87108976,\n",
      "       -0.46815524,  0.5044776 , -1.0897257 , -0.85079545, -1.3373911 ,\n",
      "       -0.38379854, -0.8757888 ,  0.18931459, -0.32886037,  0.04981094,\n",
      "        0.5398557 , -0.28116286, -0.24038386, -0.38430217, -0.7481726 ,\n",
      "        1.1256713 , -0.8327461 , -0.0557703 ,  0.6357913 , -0.8639002 ,\n",
      "        0.24770433,  0.2770964 , -0.1378254 , -0.36598012, -0.12961747,\n",
      "       -0.22794701,  0.529019  , -0.700281  , -0.77569866, -0.23197418,\n",
      "        1.0729643 , -0.08063252, -0.5232305 , -1.1956855 , -0.45231363,\n",
      "       -0.3089238 ,  0.17379457,  0.00615727, -0.62752074,  0.78913087,\n",
      "        1.1644564 , -1.1667197 , -0.21877618,  0.14915311, -1.4163064 ,\n",
      "        0.44144207, -0.16310449,  0.8748856 ,  0.74386984, -0.13676727,\n",
      "        0.28990948,  0.18444644,  1.1804513 ,  0.11773928,  0.50273305,\n",
      "        0.00254503,  0.8697307 , -1.1622422 , -0.42676902,  0.6939271 ,\n",
      "        0.63702667, -0.7512551 ,  1.1683681 , -0.13327271, -0.22244391,\n",
      "        0.15181945,  0.8999371 , -0.21500789, -0.6279617 , -0.5096534 ,\n",
      "       -0.08484695, -0.4533029 ,  1.2403706 , -0.2359471 ,  0.78793573,\n",
      "       -0.04686179, -0.02098469,  0.370362  , -0.33851078,  0.5313638 ,\n",
      "       -0.7088695 , -0.45497078,  0.26414302,  0.36852574,  0.63333416,\n",
      "        0.4654311 ], dtype=float32), array([ 0.10442144, -0.63849926, -0.7562276 ,  1.5514104 ,  0.74304926,\n",
      "        1.4898871 ,  0.28773668, -0.02360575,  1.2898352 ,  0.48345745,\n",
      "        1.0615851 , -0.2734949 ,  0.6855729 , -1.946016  , -1.5446222 ,\n",
      "       -0.6716012 , -0.9153206 ,  0.04103784, -0.74179095,  0.1289661 ,\n",
      "        0.5503858 , -0.6163987 ,  0.8325962 , -0.13442975, -2.058372  ,\n",
      "       -0.20380819,  0.09776258, -0.28451428, -0.02218837, -1.1193801 ,\n",
      "       -0.07673814,  0.08226971,  0.4630497 , -1.7804087 , -1.2453213 ,\n",
      "       -0.1986995 ,  1.1421118 , -1.2820729 , -1.2343913 ,  0.9670089 ,\n",
      "        1.7478421 , -0.11353654,  1.3210866 , -1.307783  , -1.147803  ,\n",
      "        0.9520691 , -0.1424973 , -0.17611834, -0.8241822 ,  1.2186706 ,\n",
      "        0.09164772, -1.132224  ,  0.14085862,  0.56598663, -1.6495814 ,\n",
      "        2.1442456 ,  0.11565442,  0.65653354,  1.0670918 ,  0.01686309,\n",
      "       -0.10306607, -0.7451959 ,  2.4898038 ,  0.9363219 ,  0.54222965,\n",
      "        0.42505985, -0.45138496, -1.5643555 , -1.1314411 ,  0.3303607 ,\n",
      "        0.14459957, -1.2474456 ,  1.5059723 ,  0.6590584 , -0.38523406,\n",
      "        0.60024405,  1.121708  , -0.5283759 ,  0.57201314, -0.19955415,\n",
      "        1.6955568 , -1.0985138 ,  0.24880463, -0.7285225 ,  1.8108957 ,\n",
      "        0.90435153, -0.13119653, -0.17943066, -1.0796516 ,  0.95447636,\n",
      "       -1.3756086 , -0.09097179,  0.8171397 , -0.9512716 ,  2.4646778 ,\n",
      "        0.6941892 ], dtype=float32), array([-0.26354414, -0.29535243, -0.17120856,  0.421297  ,  0.70752305,\n",
      "        0.606739  ,  1.1438358 ,  0.0406599 ,  1.0201699 ,  0.02615898,\n",
      "        0.20499931, -0.11920922, -0.40254492, -0.707029  , -1.7634279 ,\n",
      "       -1.2907393 , -0.10677437,  0.3362702 ,  0.49079594, -0.21082011,\n",
      "        0.2540276 , -0.5665128 , -0.02156894,  0.10353842, -0.71253747,\n",
      "        0.5179945 , -1.4616269 , -0.5571732 ,  0.20349486, -1.077738  ,\n",
      "        0.23266579,  0.15271063,  0.7162609 ,  0.12981027, -0.7537753 ,\n",
      "       -1.215809  ,  1.2510546 , -0.64454913, -0.18973356,  0.35638344,\n",
      "        1.3519033 ,  0.00524587, -0.09364347, -2.0200374 , -0.48554537,\n",
      "        0.01933846, -0.13353142, -0.9055476 , -0.29779145,  0.42826694,\n",
      "        0.12899734, -1.0387653 , -0.38453013, -0.36534926, -1.3085272 ,\n",
      "        0.54844755,  0.05408529,  0.9933089 ,  0.8674359 ,  0.09475691,\n",
      "        0.41994503,  0.657465  ,  1.2934808 , -0.04176804,  0.8953513 ,\n",
      "       -0.65834624,  1.0283785 , -1.2693124 , -0.8067294 ,  1.0892314 ,\n",
      "        1.0842062 , -0.9013538 ,  1.2057496 ,  0.25621545,  0.26166674,\n",
      "        0.07671968,  1.2020693 ,  0.31799293,  0.49110723,  0.23514016,\n",
      "        0.46596006, -0.31524187,  0.40137017,  0.23857753,  1.4896623 ,\n",
      "        0.07431027, -0.380736  , -0.29903394, -0.37223428,  0.12403779,\n",
      "        0.07229645, -0.94167113, -0.02348273, -0.45187676,  0.73214114,\n",
      "        0.7855737 ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Let's use spacy pretrained word embeddings models trained on english wikipedia text to create document vectors\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm',download=True) # here en_core_web_sm is small pretrained word embedding model, en_core_web_md -> medium size pretrained model and en_core_web_lg -> large size pretrained model\n",
    "\n",
    "# Here for each word in sentence particular value is being assigned as per spacy pretrained word embedding model\n",
    "vectors = [nlp(sent).vector for sent in docs]\n",
    "print(vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Text Features and use them in Classification Pipelines\n",
    "\n",
    "As we saw above, we have extracted text features and now are gonna classify text using classification pipelines in sklearn. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0.8277564416583107\n"
     ]
    }
   ],
   "source": [
    "# Let's load news group dataset where we have 20 types of news groups and we need to classify that\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "# More detailed documentation about fetch newsgroup dataset is here - https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "data_newsgroups  = fetch_20newsgroups(subset='train') # Training subset of labelled data\n",
    "\n",
    "# Split Dataset\n",
    "X_train,X_test,y_train,y_test = train_test_split(data_newsgroups.data,data_newsgroups.target,test_size=0.3)\n",
    "\n",
    "# Let's use tfidf for numeric features extraction and support vector classifier to classify text\n",
    "# Let's make a classification pipeline using above two components i.e. tfidf vectorizer and support vector classifier(SVC) with linear kernel\n",
    "\n",
    "svc_tfidf = Pipeline([('tf_idf_vectorizer',TfidfVectorizer(stop_words='english',max_features=5000)),\n",
    "                      ('svc',SVC(kernel='linear'))])\n",
    "\n",
    "scores = cross_val_score(svc_tfidf,X_train,y_train,cv=2).mean()\n",
    "print('Score',scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_tfidf.fit(X_train,y_train)\n",
    "preds = svc_tfidf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score 0.8771723122238586\n",
      "Classification Report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91       144\n",
      "           1       0.77      0.82      0.80       177\n",
      "           2       0.79      0.81      0.80       197\n",
      "           3       0.68      0.77      0.72       178\n",
      "           4       0.84      0.81      0.83       168\n",
      "           5       0.85      0.87      0.86       168\n",
      "           6       0.84      0.84      0.84       159\n",
      "           7       0.90      0.86      0.88       199\n",
      "           8       0.95      0.95      0.95       173\n",
      "           9       0.95      0.86      0.90       175\n",
      "          10       0.94      0.97      0.96       184\n",
      "          11       0.97      0.93      0.95       155\n",
      "          12       0.73      0.79      0.76       166\n",
      "          13       0.92      0.93      0.93       175\n",
      "          14       0.97      0.93      0.95       196\n",
      "          15       0.89      0.94      0.91       187\n",
      "          16       0.96      0.91      0.93       174\n",
      "          17       0.97      0.95      0.96       174\n",
      "          18       0.89      0.89      0.89       128\n",
      "          19       0.90      0.76      0.83       118\n",
      "\n",
      "    accuracy                           0.88      3395\n",
      "   macro avg       0.88      0.88      0.88      3395\n",
      "weighted avg       0.88      0.88      0.88      3395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_score_ = accuracy_score(y_test,preds)\n",
    "classification_report_ = classification_report(y_test,preds)\n",
    "\n",
    "print('Accuracy Score', accuracy_score_)\n",
    "print('Classification Report',classification_report_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis(LSA) for Document Classification\n",
    "LSA is an approach to find hidden topics that exists across set of documents. \n",
    "\n",
    "The two main steps in this process is to build term-document matrix and then reducing the dimensionality of that matrix. To reduce dimensions and discover topics we will use SVD(Support Vector Decomposition).\n",
    "\n",
    "Let's go to brief walkthrough, how we can do it using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use same dataset as above for document classification\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "categories = ['talk.religion.misc','comp.graphics'] # Let's consider 2 categories out of 20 categories of newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',categories=categories,remove=('headers','footers','quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',categories=categories,remove=('headers','footers','quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's discover latent topics from training data and fit the model and then predict topics values from testing data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5,stop_words='english',use_idf=True,max_features=5000)\n",
    "X_train,X_test,y_train,y_test = train_test_split(newsgroups_train.data,newsgroups_train.target,test_size=0.3)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8546712802768166\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88       178\n",
      "           1       0.77      0.89      0.82       111\n",
      "\n",
      "    accuracy                           0.85       289\n",
      "   macro avg       0.85      0.86      0.85       289\n",
      "weighted avg       0.86      0.85      0.86       289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's project tfidf vectors to principle components and determine topics\n",
    "svd = TruncatedSVD(100)\n",
    "lsa = make_pipeline(svd,Normalizer(copy=False))\n",
    "\n",
    "# Project the training data to lower dimensions using SVD\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "# Let's apply transformations to testing data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "X_test_lsa = lsa.transform(X_test_tfidf)\n",
    "\n",
    "# Build classifier model- e.g. KnnClassifier\n",
    "knn_classify = KNeighborsClassifier()\n",
    "knn_classify.fit(X_train_lsa,y_train)\n",
    "\n",
    "preds = knn_classify.predict(X_test_lsa)\n",
    "score = accuracy_score(y_test,preds)\n",
    "classify_report = classification_report(y_test,preds)\n",
    "print(score)\n",
    "print(classify_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare various vectorization methods for document classification\n",
    "\n",
    "Features in any Machine Learning algorithms are generally numerical data on which we can easily perform any mathematical operations. But Machine Learning algorithms cannot work on raw text data. Machine Learning algorithms can only process numerical representation in form of vector(matrix) of actual text. For converting textual data into numerical representation of features we can use the following text vectorization techniques in Natural Language Processing.\n",
    "\n",
    "* Bag Of Words (Count Vectorizer)\n",
    "* Term Frequency and Inverse Document Frequency (TF-IDF)\n",
    "* Word2Vec\n",
    "Raw data contains numerical values, punctuations, spaces, special characters which can hamper the performance of model. So it is necessary to pre-process the data first. For that we can use various pre-processing techniques like :\n",
    "\n",
    "Regular expressions â€“ for removing numerical values, punctuationâ€™s, special characters etc.\n",
    "Lowercase the text data\n",
    "Tokenization â€“ converting group of sentence into tokens\n",
    "Removing stopwords from text data. Example â€“ of, in, the etc.\n",
    "Stemming and/or Lemmetization â€“ reducing a word to its word stem\n",
    "After applying these pre-processing technique we need to convert the final extracted features into numerical features in order to build our model. This is where Text Data Vectorization techniques come into picture.\n",
    "\n",
    "Letâ€™s have a look at each of them in detail:\n",
    "\n",
    "* Bag Of Words\n",
    "\n",
    "BOW is a text vectorization model commonly useful in document representation method in the field of information retrieval.\n",
    "\n",
    "In information retrieval, the BOW model assumes that for a document, it ignores its word order, grammar, syntax and other factors, and treats it as a collection of several words. The appearance of each word in the document is independent of whether other words appear. (Itâ€™s out of order)\n",
    "The Bag-of-words model (BOW model) ignores the grammar and word order of a text, and uses a set of unordered words to express a text or a document.\n",
    "\n",
    "\n",
    "In the example above three sentences are taken which have in all 12 unique words. The order of words is not related in which they appear in sentence. The sentences are transformed to vectors using CountVectorizer() function. The output contains a total of 12 elements, where the i-th element represents the number of times the i-th word in the dictionary appears in the sentence.\n",
    "\n",
    "Imagine a huge document set D with a total of M documents. Unique words from documents are extracted, comprising a list N words. In Bag of words model, each document represents N-dimensional vector.\n",
    "\n",
    "The BOW model can be considered as a statistical histogram. It is used in text retrieval, document classification and processing applications.\n",
    "\n",
    "* TF-IDF (Term Frequency â€“ Inverse Document Frequency)\n",
    "\n",
    "Another popular word embedding/text vectorization technique for extracting features from data is TF-IDF. TF-IDF is numerical statistical technique and used to figure out the relevance of any word in document, which is part of an even larger body of document.\n",
    "\n",
    "The two metrics TF and IDF are as follows:\n",
    "\n",
    "Term Frequency (TF) â€“ In TF , we are giving some scoring for each word or token based on the frequency of that word. The frequency of a word is dependent on the length of the document. Means in large size of document a word occurs more than a small or medium size of the documents.\n",
    "\n",
    "So to overcome this problem we will divide the frequency of a word with the length of the document (total number of words) to normalize.By using this technique, we are creating a sparse matrix with frequency of every word in each document.\n",
    "\n",
    "TF = no. of times term occurrences in a document / total number of words in a document\n",
    "\n",
    "\n",
    "Inverse Document Frequency (IDF) â€“ It is a measure of the general importance of a word. The main idea is that if there are fewer documents containing the entry t and the larger, it means that the entry has a good ability to distinguish categories. The IDF of a specific word can be calculated by dividing the total number of files by the number of files containing the word, and then taking the log of the obtained quotient.\n",
    "\n",
    "IDF = log base e (total number of documents / number of documents which are having term t)\n",
    "\n",
    "\n",
    "Example:\n",
    "Consider a document containing 100 words where in the word cat appears 3 times. The term frequency (Tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these.\n",
    "\n",
    "Then, the inverse document frequency (Idf) is calculated as log(10,000,000 / 1,000) = 4.\n",
    "\n",
    "Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
    "\n",
    "\n",
    "In this code the TF-IDF of three sentences is found and converted to vectors using TfidfVectorizer(). TF-IDF value increases based on frequency of the word in a document. Like Bag of Words in this technique also we can not get any semantic meaning for words.\n",
    "\n",
    "* TF-IDF application\n",
    "Search Engine\n",
    "Keyword Extraction\n",
    "Text Similarity\n",
    "Text Summary\n",
    "Word2Vec\n",
    "\n",
    "With Bag of Words and TF-IDF text vectorization techniques we did not get semantic meaning of words. But for most of the applications of NLP tasks like sentiment classification, sarcasm detection etc require semantic meaning of a word and semantic relationships of a word with other words.\n",
    "\n",
    "Word embeddings captures semantic and syntactic relationships between words and also the context of words in a document. Word2vec technique used to implement word embeddings.\n",
    "\n",
    "* Word2vec model takes input as a large size of corpus and produces output to vector space. This vector space size may be in hundred of dimensionality. Each word vector will be placed on this vector space. In vector space words that share context commonly in a corpus are closer to each other. Word vector having positions of corresponding words in a vector space.\n",
    "\n",
    "The Word2vec method learns all those types of relationships of words while building a model. For this purpose word2vec uses 2 types of methods.\n",
    "\n",
    "Skip-gram\n",
    "CBOW (Continuous Bag of Words)\n",
    "The Word2vec model captures relationships of words with the help of window size by using skip-gram and CBOW methods. Window size is a technique similar to n-grams where we create sequence of n words.\n",
    "\n",
    "\n",
    "* Skip-gram\n",
    "Skip-gram method takes the center word from the window size words as an input and context words (neighbour words) as outputs. Word2vec models predict the context words of a center word using skip-gram method. Skip-gram works well with a small dataset and identifies rare words really well.\n",
    "\n",
    "* Continuous Bag-of-Words (CBOW)\n",
    "CBow is just a reverse method of the skip gram method. Here we are taking context words as input and predicting the center word within the window. Another difference from skip gram method is, It was working faster and better representations for most frequency words.\n",
    "\n",
    "\n",
    "Word2Vec has its applications in knowledge discovery and recommendation systems.\n",
    "\n",
    "Conclusion:\n",
    "We can use any one of the text feature extraction based on our project requirement. Because every method has their advantages  like a Bag-Of-Words suitable for text classification, TF-IDF is for document classification and if you want semantic relation between words then go with word2vec.\n",
    "\n",
    "We canâ€™t say blindly what type of feature extraction gives better results. One more thing is building word embeddings from our dataset or corpus will give better results. But we donâ€™t always have enough size of data set so in that case we can use pre-trained models with transfer learning."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b743127f8f4c955843e2abebe4405e9e9befcc023b2719b748b063e552479ec"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
